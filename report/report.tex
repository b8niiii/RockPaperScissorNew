% !TEX root = report.tex

\documentclass[a4paper,11pt]{article}

\usepackage{graphicx}
\graphicspath{{./outputs_imgs/}}

\usepackage[utf8]{inputenc}   % per scrivere in UTF-8 (accenti)
\usepackage[T1]{fontenc}
\usepackage[english]{babel}   % o [italian] se scrivi in italiano
\usepackage{graphicx}         % per inserire immagini
\usepackage{amsmath, amssymb} % per formule matematiche
\usepackage{hyperref}         % per link cliccabili
\usepackage{caption}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{booktabs}



\title{Rock, Paper and Scissors Image Classification}
\author{Alessandro Bottoni}
\date{\today} 

\begin{document}

\maketitle

\section{Introduction and Dataset}
The following project aims at developing a Convolutional Neural Network capable of recognizing rock, paper and scissors hand gestures. \newline
It consists of the development of three different models, with increasing complexity and in the comparison of their performance.  \newline
The dataset is composed of 2189 pictures in a .png format and divided in three classes: \newline
1. Rock, with 726 pictures.\newline
2. Paper, with 712 pictures.\newline
3. Scissors, with 750 pictures.\newline

\section{Preprocessing and Data Augmentation}
% Spiega resize a 256x256, normalizzazione, e le augmentations (flip, rotation, jitter).
To prepare the dataset for training, a series of preprocessing steps and data augmentation techniques were applied. First, all images were resized to a uniform size of 256×256 pixels to ensure consistent input dimensions for the CNN. Next, pixel values were normalized to the range by dividing by 255, which supports faster convergence during training.
To increase training-data diversity and improve model generalization, several augmentation techniques were used. These included random horizontal flips to reflect natural variability in hand gestures and random rotations within ±15° to account for different hand orientations. In addition, random color jittering was applied to vary brightness, contrast, and saturation, improving robustness to changing lighting conditions.
\newline
\newline

% Spiega lo script che divide per classe in 70/15/15 (o 80/10/10) e la struttura delle cartelle.
Altough it is a best practice to use cross validation to validate models, in order to avoid overfitting on the validation set, for the first model a simple train/validation/test split was used. 
This choice was made to speed up the training process, which is already quite long with the current hardware. The dataset was split into 70\% for training, 15\% for validation and 15\% for testing. 
The split was performed separately for each class to ensure that the distribution of classes is consistent across all sets. 
As for the other two models a cross validation approach has been used, to better evaluate the performance of the models and avoid overfitting on the validation set.

\section{Simple CNN Model and Training}
% Descrivi il modello (CNN), gli hyperparameters e come usi i DataLoader.
\subsection{Model Architecture: Simple CNN} 

The first model designed is a very simple Convolutional Neural Network (CNN) architecture. 
It consists of a single convolutional layer followed by a ReLU activation function, a pooling layer, a flatten layer and a fully connected one.
The convolutional layer takes as input 3 channels (RGB color images), the only fixed parameter, while the number of output channels and the kernel size are hyperparameters that can be tuned in the following way: \newline \newline
- Number of output channels: 16, 32, 64 \newline 
- Kernel size: 3, 4, 7 \newline

The padding is automatically calculated as the kernel size divided by 2, to preserve the spatial dimensions of the input. The purpose of this layer is to extract spatial features from the input images, such as edges and textures. 
The ReLU activation function introduces non-linearity to the model, allowing it to learn more complex patterns, while the pooling layer reduces the spatial dimensions of the feature maps, which helps to reduce the number of parameters and computational cost.
The kernel size of the pooling layer is tunable, and can be set to either 2 or 4, where 2 preserves more details, and 4 is faster but loses information.
Finally, the flatten layer converts the 2D feature maps into a 1D vector, which is then passed to the fully connected layer for classification. \newline

\subsection{Model Training Methodology}

\subsubsection{Hyperparameter Search Space and Strategy}

The project employs Ray Tune, a distributed hyperparameter optimization framework, to systematically explore the hyperparameter space. Ray Tune provides efficient resource management, parallel trial execution, and sophisticated scheduling algorithms for early stopping.

\subsubsection{Architecture Hyperparameters (Grid Search)}

Architecture-related hyperparameters are explored using exhaustive grid search to ensure comprehensive evaluation of model capacity:

\begin{itemize}
    \item \textbf{Output Channels} $\in \{8, 16, 32\}$: Controls the number of filters in the convolutional layer, directly affecting model capacity. Lower values reduce parameters and computational cost, while higher values increase representational power.
    
    \item \textbf{Convolutional Kernel Size} $\in \{3, 4, 7\}$: Determines the receptive field of each filter. Smaller kernels capture fine-grained local features, while larger kernels capture broader spatial context. The sizes were chosen to represent small ($3 \times 3$), medium ($4 \times 4$), and large ($7 \times 7$) receptive fields.
\end{itemize}

Grid search generates $3 \times 3 = 9$ unique architecture combinations.

\subsubsection{Training Hyperparameters (Random Search)}

Training-related hyperparameters are sampled randomly to balance exploration efficiency with computational cost:

\begin{itemize}
    \item \textbf{Pooling Kernel Size} $\in \{2, 4\}$: Sampled using \texttt{tune.choice}. Controls the downsampling factor. A kernel size of 2 is standard practice, preserving more spatial detail, while a kernel size of 4 performs aggressive downsampling, trading spatial resolution for faster computation.
    
    \item \textbf{Batch Size} $\in \{16, 32, 64\}$: Sampled uniformly with equal probability. Smaller batch sizes provide noisier gradient estimates but often generalize better, while larger batch sizes offer more stable training and faster computation per epoch.
\end{itemize}
\subsubsection{Training Hyperparameters (Log-Uniform)}
\begin{itemize}
    \item \textbf{Learning Rate}: Sampled from a log-uniform distribution. Log-uniform sampling ensures equal representation across orders of magnitude, as learning rates often require logarithmic exploration. Mathematically:
\end{itemize}

With \texttt{num\_samples=10}, each of the 9 architecture combinations is evaluated with 10 different random samples of training hyperparameters, yielding a maximum of $9 \times 10 = 90$ potential trials.
\subsubsection{Ray Tune Configuration and Training Loop}

Before describing the tuning workflow, it is useful to summarize the training setup at a high level. The model is trained using stochastic gradient descent with momentum, optimized against a cross-entropy loss for a three-class classification task. Each trial runs for a limited number of epochs (up to 10), and the batch size is treated as a tunable parameter, so the amount of data processed per update can change across trials. Data are fed through separate training and validation loaders: the training loader shuffles samples to improve generalization, while the validation loader keeps a fixed order to make evaluation stable; a small number of worker processes is used to overlap data loading with computation.

In this project, hyperparameter tuning is organized as a collection of independent \emph{trials}. Each trial corresponds to one concrete set of hyperparameters and runs a short training job, producing comparable validation metrics that Ray Tune can track across experiments. The overall goal is to explore many configurations efficiently while keeping the execution stable and reproducible.

To keep the machine responsive, the tuning process is run under a controlled resource budget. Instead of letting every trial consume as many resources as it wants, each trial is assigned a fixed amount of compute (a certain number of CPU cores). This explicit resource assignment determines how many trials can run at the same time: if each trial needs more CPU, fewer trials can run in parallel; if each trial needs less, more trials can run concurrently. Ray Tune supports this kind of per-trial resource allocation so that parallelism is predictable and does not overwhelm the system. \cite{ray_tuning}

To avoid wasting time on bad configurations, the search uses the ASHA scheduler (Asynchronous Successive Halving). ASHA implements an early-stopping policy that continuously compares trials as they progress: at predefined milestones in training, a trial's performance is evaluated against other trials that have reached the same point. Trials that are clearly underperforming are stopped early, and the freed resources are immediately reused to start new trials or continue more promising ones. Because decisions are made asynchronously, trials do not need to wait for each other, which improves resource utilization and speeds up the overall search. \cite{ray_tuning}

Checkpointing is used to preserve useful model states during tuning. Rather than saving many snapshots, the configuration can be set up to retain only the most relevant checkpoint per trial (typically the best one according to the chosen validation metric). This makes it easier to recover the best-performing model after tuning and reduces disk usage, which is important when running many trials. \cite{ray_tuning}

Inside each trial, the training loop follows a standard pattern. Each epoch is split into a training phase (where model parameters are updated) and a validation phase (where performance is measured without updating the model). After each epoch, the trial reports key metrics (such as validation loss, validation accuracy, and training loss) back to Ray Tune so that the scheduler can make early-stopping decisions and so that results are logged in a consistent format. \cite{ray_tuning}

Finally, the entire experiment is orchestrated by the Tune \emph{Tuner}. The Tuner combines the search space definition with the optimization objective (which metric to minimize or maximize) and the scheduler policy. It generates trial configurations, queues them, runs them when resources are available, and collects the results into a single structured output that you can inspect to select the best configuration and its corresponding checkpoint. \cite{ray_tuning}

% Quando li hai: accuracy, qualche grafico, una confusion matrix, breve commento: metti accuracy, precision, recall, f1 score e grafico che mostra andamento accuracy e andamento loss.

% -----------------------------
% Results — Model 1 (Simple CNN)
% -----------------------------
\subsection{ Simple CNN (Ray Tune)'s Results}

\paragraph{Best hyperparameters:} 
Ray Tune selected the following configuration that turned out to the best by minimum validation loss across all trials::
\begin{itemize}
  \item $out\_channels = 32$
  \item $conv\_kernel\_size = 7$
  \item $pool\_kernel\_size = 4$
  \item $lr = 7.6444576 \times 10^{-4}$
  \item $batch\_size = 16$
\end{itemize} % [file:60]

\paragraph{Performance:}
For the selected trial, the final validation loss was $0.2784$ and the final validation accuracy was $0.9202$. % [file:60]
While on the test set, over $n=331$ images, the model achieved test loss $0.2737$ and test accuracy $0.9154$. % [file:60] 
\newline It is important to point out that due to the lack of cross-validation, these results are highly dependent on the specific train/validation/test split used, and may not be fully representative of the model's generalization performance. % [file:60]
\begin{figure}[htbp]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{model1_val_loss.png}
    \caption{Model 1: validation loss across epochs (best Ray Tune trial).}
    \label{fig:model1-val-loss}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{model1_val_accuracy.png}
    \caption{Model 1: validation accuracy across epochs (best Ray Tune trial).}
    \label{fig:model1-val-acc}
  \end{subfigure}
  \caption{Learning curves for the best Model 1 configuration.}
  \label{fig:model1-learning-curves}
\end{figure}

\paragraph{Confusion matrix:}
Figure~\ref{tab:model1-cm-counts} reports the confusion matrix on the test set (rows = true class, columns = predicted class). % [file:60]

\begin{table}[htbp]
  \centering
  \caption{Model 1 confusion matrix counts on the test set (rows: true class; columns: predicted class).}
  \label{tab:model1-cm-counts}
  \begin{tabular}{lccc}
    \toprule
      & Pred: Paper & Pred: Rock & Pred: Scissors \\
    \midrule
    True: Paper    & 98 & 1  & 9   \\
    True: Rock     & 2  & 95 & 13  \\
    True: Scissors & 1  & 2  & 110 \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Conclusion (Model 1).}
Despite its minimal architecture, Model~1 provides a strong baseline for the task: the best Ray Tune configuration achieves $0.920$ validation accuracy and $0.915$ test accuracy (test loss $0.274$ on $n=331$ images). %
The learning curves indicate rapid convergence within the 10 training epochs used during tuning, with validation loss decreasing and accuracy stabilizing near its final plateau, suggesting stable optimization. %
The confusion matrix shows that most predictions fall on the diagonal (98, 95, 110 correct per class), while the remaining errors are concentrated between visually similar gestures (notably some Rock and Paper samples predicted as Scissors). %
Overall, this computationally efficient model seems to already generalize well, making it a solid reference point for the upcoming comparison with deeper architectures and cross-validation.


% ------------------------------------------------------------
% Model 2: Medium CNN (with Cross-Validation)
% ------------------------------------------------------------
\section{Medium CNN Model and Training}

\subsection{Training procedure}
In the second experiment we moved from the baseline (Model 1) to a moderately deeper CNN architecture, while keeping the same overall input pipeline and classification goal (3 classes: rock, paper, scissors).% [file:2][file:1]
\newline Differently from Model 1, here the model assessment and model selection were carried out using \textbf{K-fold cross-validation}, in order to obtain a more reliable estimate of the generalization performance and to reduce dependence on a single train/validation split.% [file:2][file:1]
The preprocessing  and data augmentation pipeline is the same as in the model 1, with the exception of the train/validation split, which is now performed within each fold of the cross-validation procedure.% [file:2][file:1]
\subsubsection{Model Architecture}
Model 2 increases capacity by stacking two convolutional blocks and introducing explicit regularization (Batch Normalization and Dropout).% [file:1]
The network is composed of a \texttt{features} module (convolutions, normalization, ReLU and pooling) and a \texttt{classifier} module (fully-connected layers).% [file:1]

\paragraph{Feature extractor:}
\begin{itemize}
  \item \textbf{Block 1:} \texttt{Conv2d(3 $\rightarrow$ 16, kernel=7, stride=1, padding=3)} $\rightarrow$ \texttt{BatchNorm2d(16)} $\rightarrow$ \texttt{ReLU} $\rightarrow$ \texttt{MaxPool2d(2)}.% [file:1]
  \item \textbf{Block 2:} \texttt{Conv2d(16 $\rightarrow$ 32, kernel=3, padding=1)} $\rightarrow$ \texttt{BatchNorm2d(32)} $\rightarrow$ \texttt{ReLU} $\rightarrow$ \texttt{MaxPool2d(2)}.% [file:1]
\end{itemize}

\paragraph{Classifier:}
After the convolutional blocks, the feature maps are flattened and passed to:% [file:1]
\begin{itemize}
  \item \texttt{LazyLinear(128)} $\rightarrow$ \texttt{ReLU} $\rightarrow$ \texttt{Dropout(0.5)} $\rightarrow$ \texttt{Linear(128, 3)}.% [file:1]
\end{itemize}
The final layer outputs logits for the three-class classification task.% [file:1]

\subsubsection{Training setup}
The model is trained with cross-entropy loss (\texttt{CrossEntropyLoss}) and optimized using stochastic gradient descent with momentum (\texttt{SGD}, momentum $=0.9$).% [file:1]
We fix the main hyperparameters to the best configuration previously found while experimenting with Model 1 (learning rate $\approx 5.12 \times 10^{-4}$, batch size $=16$), and we do not run a new tuning procedure for Model 2. \newline % [file:1]
Each training run lasts $20$ epochs.% [file:1]

To improve reproducibility, we set a global seed (\texttt{SEED}=42) for Python, NumPy, and PyTorch, and we enable deterministic behavior where applicable.% [file:1]
Training is executed on Apple Silicon \texttt{MPS} % [file:1]

\subsubsection{K-fold cross-validation protocol}
To perform cross-validation, we combine the available training and validation folders into a single dataset and then split it using \texttt{KFold} with $K=10$, shuffling enabled and \texttt{random\_state}=42.% [file:1]
\newline For each fold, the model is trained \emph{from scratch} on $K-1$ folds and validated on the remaining fold, repeating the process until each fold has served as validation exactly once.% [file:1] 
\newline Within each fold, we build a shuffled training dataloader and a non-shuffled validation dataloader (batch size $=16$, \texttt{num\_workers}=2).% [file:1]
\newline During training we log training and validation loss/accuracy at each epoch, and we track the best validation loss and best validation accuracy observed within each fold.% [file:1]
\newline After completing the $10$ folds, we retrain a final instance of the same architecture on the full (train+validation) dataset using the selected training configuration, and we save the resulting weights for the final test evaluation.% [file:1]
% Quando li hai: accuracy, qualche grafico, una confusion matrix, breve commento: metti accuracy, precision, recall, f1 score e grafico che mostra andamento accuracy e andamento loss.

% -----------------------------
% Results — Model 2 (Medium CNN)
% -----------------------------
\subsection{Results for Medium CNN with K-fold Cross-Validation}

\paragraph{Cross-validation performance (K-fold):}
Across the $10$ folds, the mean \emph{best} validation accuracy was $0.9596 \pm 0.0147$ (mean $\pm$ std), while the mean \emph{best} validation loss was $0.1294 \pm 0.0352$. % [file:92]
Figure~\ref{fig:model2-cv-curves} reports the learning curves aggregated across folds (mean $\pm$ std), showing stable convergence and limited variance across splits. % [file:92]

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{outputCV2-LossAcrossFolds.png}
        \caption{Model 2: loss across folds (mean $\pm$ std) over epochs.}
        \label{fig:model2-cv-loss}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{outputCV2-AccAcrossFolds.png}
        \caption{Model 2: accuracy across folds (mean $\pm$ std) over epochs.}
        \label{fig:model2-cv-acc}
    \end{subfigure}
    \caption{Aggregated cross-validation learning curves for Model 2.}
    \label{fig:model2-cv-curves}
\end{figure}

\paragraph{Final test performance:}
After cross-validation, a final instance of Model~2 was retrained on the full (train+validation) dataset and evaluated on the held-out test set. % [file:92]
On the test set (classes: \texttt{paper}, \texttt{rock}, \texttt{scissors}, $n=331$), the model achieved test loss $0.1294$ and test accuracy $0.9668$. % [file:92]

\paragraph{Confusion matrix:}
Figure~\ref{fig:model2-cm} shows the confusion matrix on the test set (rows = true class, columns = predicted class), with the corresponding raw counts reported in Table~\ref{tab:model2-cm-counts}. % [file:92]

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.62\textwidth]{model2_confusion_matrix.png}
    \caption{Model 2: confusion matrix on the test set.}
    \label{fig:model2-cm}
\end{figure}

\begin{table}[htbp]
    \centering
    \caption{Model 2 confusion matrix counts on the test set (rows: true class; columns: predicted class).}
    \label{tab:model2-cm-counts}
    \begin{tabular}{lccc}
        \toprule
        & Pred: Paper & Pred: Rock & Pred: Scissors \\
        \midrule
        True: Paper    & 100 & 1  & 7   \\
        True: Rock     & 0   & 109 & 1  \\
        True: Scissors & 0   & 2  & 111 \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{Precision/Recall/F1 (test set).}
Table~\ref{tab:model2-cls-report} reports precision, recall, and F1-score per class on the test set (support shown in the last column). % [file:92]
Overall, the macro-averaged F1-score was $0.9668$ and the weighted-averaged F1-score was $0.9667$. % [file:92]

\begin{table}[htbp]
    \centering
    \caption{Model 2 classification report on the test set.}
    \label{tab:model2-cls-report}
    \begin{tabular}{lcccc}
        \toprule
        Class & Precision & Recall & F1-score & Support \\
        \midrule
        Paper    & 1.0000 & 0.9259 & 0.9615 & 108 \\
        Rock     & 0.9732 & 0.9909 & 0.9820 & 110 \\
        Scissors & 0.9328 & 0.9823 & 0.9569 & 113 \\
        \midrule
        Accuracy & \multicolumn{3}{c}{0.9668} & 331 \\
        Macro avg    & 0.9687 & 0.9664 & 0.9668 & 331 \\
        Weighted avg & 0.9681 & 0.9668 & 0.9667 & 331 \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{Conclusion (Model 2).}
Model~2 achieves strong and consistent performance under cross-validation, with mean best validation accuracy $0.9596$ and relatively low fold-to-fold variability. % [file:92]
After retraining on the full development set, the final model generalizes well to the held-out test set, reaching $0.9668$ accuracy with balanced precision/recall across the three classes. % [file:92]
Most residual errors are limited and class-specific (e.g., a small number of \texttt{paper} samples predicted as \texttt{scissors}), as shown by the confusion matrix and per-class metrics. % [file:92]


% ------------------------------------------------------------
% Model 3: Residual CNN (with Cross-Validation)
% ------------------------------------------------------------
\section{Residual CNN Model and Training}

\subsection{Residual CNN and training procedure}
In the third experiment we further increased model capacity by adopting a residual CNN architecture, i.e., a network built from residual blocks with skip connections, while keeping the same three-class classification objective (rock, paper, scissors). % [file:44]
As for Model 2, model assessment is performed using \textbf{K-fold cross-validation}, which reduces dependence on a single train/validation split and provides a more stable estimate of generalization performance. % [file:44]
The preprocessing and data augmentation pipeline is the
same as in the model 1, with the exception of the train/validation split,
which is now performed within each fold of the cross-validation procedure.
\subsubsection{Residual blocks (BasicBlock)}
Model 3 is based on residual learning, where the output of a transformation $F(\cdot)$ is added to the original input through a shortcut connection. % [web:35]
In its simplest form, a residual block can be expressed as
\[
y = F(x) + x,
\]
which helps the optimization of deeper networks by allowing gradients to flow through identity paths. % [web:35]

In our implementation, each residual block (\texttt{BasicBlock}) contains two $3\times3$ convolutional layers, each followed by batch normalization, with a ReLU non-linearity after the first convolution and again after the residual addition. % [file:44]
To introduce regularization, the block can optionally apply a spatial dropout (\texttt{Dropout2d}) after the first activation (with a drop probability set per stage). % [file:44]
When the number of channels changes or spatial downsampling is required (i.e., \texttt{stride} $\neq 1$), the shortcut branch uses a projection consisting of a $1\times1$ convolution with matching stride followed by batch normalization, so that tensor dimensions align before the summation. % [file:44][web:31]

\subsubsection{Architecture (ResCNN)}
The network is organized into: (i) an initial convolutional stem, (ii) three residual stages, and (iii) a classification head. % [file:44]

\paragraph{Stem:}
The stem is composed of \texttt{Conv2d(3 $\rightarrow$ 16, kernel=7, stride=1, padding=3)} $\rightarrow$ \texttt{BatchNorm2d(16)} $\rightarrow$ \texttt{ReLU} $\rightarrow$ \texttt{MaxPool2d(2)}. % [file:44]

\paragraph{Residual stages.}
\begin{itemize}
  \item \textbf{Stage 1 (16 channels, no downsampling):} \texttt{BasicBlock(16 $\rightarrow$ 16, stride=1, drop=0.0)} $\times 2$. % [file:44]
  \item \textbf{Stage 2 (32 channels, downsampling):} \texttt{BasicBlock(16 $\rightarrow$ 32, stride=2, drop=0.1)} followed by \texttt{BasicBlock(32 $\rightarrow$ 32, stride=1, drop=0.1)}. % [file:44]
  \item \textbf{Stage 3 (64 channels, downsampling):} \texttt{BasicBlock(32 $\rightarrow$ 64, stride=2, drop=0.2)} followed by \texttt{BasicBlock(64 $\rightarrow$ 64, stride=1, drop=0.2)}. % [file:44]
\end{itemize}

\paragraph{Classifier.}
After the residual stages, features are flattened and passed to \texttt{LazyLinear(128)} $\rightarrow$ \texttt{ReLU} $\rightarrow$ \texttt{Dropout(0.5)} $\rightarrow$ \texttt{Linear(128, 3)}. % [file:44]
The output layer produces logits for the three target classes. % [file:44]

\subsubsection{Training setup}
Training uses a multi-class cross-entropy loss (\texttt{CrossEntropyLoss}) and stochastic gradient descent with momentum (\texttt{SGD}, momentum $=0.9$). % [file:44]
We keep the learning rate fixed to $5.115859\times 10^{-4}$ and use a batch size of $16$, training for $20$ epochs per fold. % [file:44]
To improve reproducibility, we set a global seed (\texttt{SEED}=42) for Python, NumPy, and PyTorch, and enable deterministic behavior where applicable. % [file:44]
Training runs on Apple Silicon \texttt{MPS}. % [file:44]

\subsubsection{K-fold cross-validation protocol}
Cross-validation is implemented by merging the existing training and validation folders into a single dataset (\texttt{ConcatDataset}) and then applying \texttt{KFold} with $K=5$, shuffling enabled, and \texttt{random\_state}=42. % [file:44]
For each fold, the model is trained \emph{from scratch} on $K-1$ folds and validated on the remaining fold, so that each sample serves as validation exactly once. % [file:44]
Within each fold, we build a shuffled training dataloader and a non-shuffled validation dataloader (batch size $=16$, \texttt{num\_workers}=0). % [file:44]
During training we log train/validation loss and accuracy at each epoch and track the best validation metrics within each fold for later aggregation (reported in the Results section). % [file:44]

After completing cross-validation, we train a final instance of the same residual architecture on the full (train+validation) dataset using the same configuration and save the weights to disk for final test evaluation (reported separately). % [file:44]
% Quando li hai: accuracy, qualche grafico, una confusion matrix, breve commento: metti accuracy, precision, recall, f1 score e grafico che mostra andamento accuracy e andamento loss.
% -----------------------------
% Results — Model 3 (Residual CNN)
% -----------------------------
\subsection{Residual CNN with 5-fold Cross-Validation: Results}

\paragraph{Cross-validation performance:}
Across the 5 folds, the mean \emph{best} validation accuracy was $0.9849 \pm 0.0036$ (mean $\pm$ std), while the mean \emph{best} validation loss was $0.0613 \pm 0.0283$. % [file:95]
Figure~\ref{fig:model3-cv-curves} reports the learning curves aggregated across folds (mean $\pm$ std), showing strong convergence and limited variance across splits. % [file:95]

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{model3_loss_mean_std.png}
    \caption{Model 3: loss across folds (mean $\pm$ std) over epochs.}
    \label{fig:model3-cv-loss}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{model3_accuracy_mean_std.png}
    \caption{Model 3: accuracy across folds (mean $\pm$ std) over epochs.}
    \label{fig:model3-cv-acc}
  \end{subfigure}
  \caption{Aggregated cross-validation learning curves for Model 3.}
  \label{fig:model3-cv-curves}
\end{figure}

\paragraph{Final test performance:}
After cross-validation, a final instance of Model~3 was retrained on the full (train+validation) dataset and evaluated on the held-out test set. % [file:95]
On the test set (classes: \texttt{paper}, \texttt{rock}, \texttt{scissors}, $n=331$), the model achieved test loss $0.0619$ and test accuracy $0.9879$. % [file:95]

\paragraph{Confusion matrix:}
Figure~\ref{fig:model3-cm} shows the confusion matrix on the test set (rows = true class, columns = predicted class), with the corresponding raw counts reported in Table~\ref{tab:model3-cm-counts}. % [file:95]

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.62\textwidth]{model3_confusion_matrix.png}
  \caption{Model 3: confusion matrix on the test set.}
  \label{fig:model3-cm}
\end{figure}

\begin{table}[htbp]
  \centering
  \caption{Model 3 confusion matrix counts on the test set (rows: true class; columns: predicted class).}
  \label{tab:model3-cm-counts}
  \begin{tabular}{lccc}
    \toprule
      & Pred: Paper & Pred: Rock & Pred: Scissors \\
    \midrule
    True: Paper    & 107 & 1   & 0   \\
    True: Rock     & 1   & 108 & 1   \\
    True: Scissors & 0   & 1   & 112 \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Precision/Recall/F1:}
Table~\ref{tab:model3-cls-report} reports precision, recall, and F1-score per class on the test set (support shown in the last column). % [file:95]
Overall, the macro-averaged F1-score was $0.9879$ and the weighted-averaged F1-score was $0.9879$. % [file:95]

\begin{table}[htbp]
  \centering
  \caption{Model 3 classification report on the test set.}
  \label{tab:model3-cls-report}
  \begin{tabular}{lcccc}
    \toprule
    Class & Precision & Recall & F1-score & Support \\
    \midrule
    Paper    & 0.9907 & 0.9907 & 0.9907 & 108 \\
    Rock     & 0.9818 & 0.9818 & 0.9818 & 110 \\
    Scissors & 0.9912 & 0.9912 & 0.9912 & 113 \\
    \midrule
    Accuracy & \multicolumn{3}{c}{0.9879} & 331 \\
    Macro avg    & 0.9879 & 0.9879 & 0.9879 & 331 \\
    Weighted avg & 0.9879 & 0.9879 & 0.9879 & 331 \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Conclusion for Model 3:}
Model~3 achieves excellent and stable performance under 5-fold cross-validation, with mean best validation accuracy $0.9849$ and low fold-to-fold variability. % [file:95]
After retraining on the full development set, the final model generalizes extremely well to the held-out test set, reaching $0.9879$ accuracy with uniformly high precision/recall across all three classes. % [file:95]
The confusion matrix confirms that errors are rare and mostly isolated (only a few samples per class are misclassified). % [file:95]

\section{Conclusion}
Across the three experiments, performance improves consistently as model capacity and architectural sophistication increase. Model~1 (a minimal single-convolution baseline tuned with Ray Tune) already achieves strong generalization, with $0.9154$ test accuracy (test loss $0.2737$ on $n=331$). Model~2, which increases depth and introduces explicit regularization (Batch Normalization and Dropout) and is assessed via 10-fold cross-validation, provides a clear gain, reaching $0.9668$ test accuracy (test loss $0.1294$) with mean best validation accuracy $0.9596 \pm 0.0147$. Finally, Model~3 (residual CNN with skip connections) achieves the best and most stable results, with $0.9879$ test accuracy (test loss $0.0619$) and mean best validation accuracy $0.9849 \pm 0.0036$ under 5-fold cross-validation. Overall, Model~2 represents an effective accuracy--complexity trade-off, while Model~3 is the best final choice when maximizing accuracy and robustness is the primary objective.


\section{Bibliography}

\begin{thebibliography}{9}

\bibitem{stride} 
Understand "stride". 
\url{https://medium.com/@bragadeeshs/stride-in-cnns-stepping-towards-efficient-image-processing-e58a34b02ff0}

\bibitem{padding} 
Understand "padding". 
\url{https://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html}

\bibitem{torch.nn}
Basic tutorials for inspiration: what is torch.nn?.
\url{https://docs.pytorch.org/tutorials/}

\bibitem{buildmodel_tutorial}
Basic tutorials for inspiration: training a Neural Network.
\url{https://docs.pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html}

\bibitem{cifar10_tutorial}
Basic tutorials for inspiration: training an image classifier.
\url{https://docs.pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html}

\bibitem{conv2d_parameters}
Understand conv2d parameters.
\url{https://www.codegenes.net/blog/conv2d-parameter-object-input-pytorch/}

\bibitem{ray_tuning}
Hyperparameter fine-tuning with Ray.
\url{https://docs.pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html}

\bibitem{sgd_momentum}
Stochastic Gradient Descent and Momentum.
\url{https://www.lunartech.ai/blog/mastering-stochastic-gradient-descent-the-backbone-of-deep-learning-optimization}

\bibitem{sgd_momentum}
SkLearn for Cross Validation.
\url{https://discuss.pytorch.org/t/how-can-i-use-sklearn-kfold-with-imagefolder/36577}

\end{thebibliography}


\par
\vspace{1cm}
\footnotesize
\textit{I declare that this material, which I now submit for assessment, is entirely my own work and has not been taken from the work of others, save and to the extent that such work has been cited and acknowledged within the text of my work. I understand that plagiarism, collusion, and copying are grave and serious offences in the university and accept the penalties that would be imposed should I engage in plagiarism, collusion or copying. This assignment, or any part of it, has not been previously submitted by me or any other person for assessment on this or any other course of study.}

\end{document}